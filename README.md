# üïµÔ∏è‚Äç‚ôÇÔ∏è PoC : Attaque Backdoor sur Sign Language MNIST

Ce projet montre une preuve de concept d'attaque par empoisonnement (BadNets) sur un classifieur de langage des signes. L'objectif est de conserver une bonne pr√©cision sur des images propres tout en for√ßant une pr√©diction choisie d√®s qu'un patch d√©clencheur est pr√©sent.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red)
![Usage](https://img.shields.io/badge/Usage-Educational-orange)

## üéØ Objectifs

- Furtivit√© : pr√©cision √©lev√©e sur les images propres.
- Efficacit√© : taux de succ√®s proche de 100 % quand le patch est pr√©sent.

## üß† D√©tails de l'attaque

- Type : Dirty Label / BadNets.
- Donn√©es : Sign Language MNIST (24 classes, J et Z exclus) ‚Äî source Kaggle : https://www.kaggle.com/datasets/datamunge/sign-language-mnist.
- Patch : carr√© 5x5 en haut √† gauche, valeur 1.0.
- Cible : classe 15 (remapp√©e).
- Taux d'empoisonnement : 5 % des images d'entra√Ænement.

## üìÇ Structure

```text
POC/
 ‚î£ data/
 ‚îÉ ‚î£ sign_mnist_train.csv
 ‚îÉ ‚îó sign_mnist_test.csv
 ‚î£ POC.py
 ‚îó README.md
```

## ‚öôÔ∏è Installation

1) Pr√©requis : Python 3.10+, drivers NVIDIA r√©cents si GPU.
2) Environnement virtuel :

```powershell
python -m venv .venv
.\.venv\Scripts\Activate
```

3) D√©pendances :

```powershell
# PyTorch (ajustez la version CUDA au besoin)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Librairies communes
pip install pandas numpy matplotlib
```

## üöÄ Utilisation

1) Placez `sign_mnist_train.csv` et `sign_mnist_test.csv` dans `data/` (ou `data/asl_data/`).
2) Lancez l'entra√Ænement et l'attaque :

```powershell
python POC.py
```

Le script :
- charge et remappe les labels (suppression du trou J=9) ;
- entra√Æne un mod√®le propre ;
- entra√Æne un mod√®le infect√© avec 5 % d'images patch√©es ;
- calcule la clean accuracy et l'Attack Success Rate ;
- affiche quelques exemples patch√©s pour v√©rifier le trigger.

## üìä R√©sultats attendus

- Clean Accuracy (furtivit√©) : > 90 % si l'entra√Ænement converge correctement.
- Attack Success Rate (efficacit√©) : souvent proche de 100 % lorsque le patch est visible.

Illustration : exemples infect√©s (validation)

![Images infect√©es (patch en haut √† gauche)](images/Infected%20pictures.png)

## üõ†Ô∏è Notes techniques

- Mod√®le : CNN avec 3 blocs conv + BatchNorm + Dropout, puis deux couches denses.
- Sp√©cificit√©s Windows/CUDA : `drop_last=True` pour la stabilit√© des batchs, pas de `torch.compile`.
- Remapping labels : les labels > 9 sont d√©cal√©s de -1 pour avoir 24 classes continues (0-23).

Illustration du mod√®le (vue verticale ONNX)

![Architecture du mod√®le](images/backdoor_model.onnx.svg)

## ‚ö†Ô∏è Avertissement √©thique

Projet fourni uniquement √† des fins √©ducatives et de recherche. Toute utilisation offensive ou non autoris√©e est ill√©gale et non encourag√©e.